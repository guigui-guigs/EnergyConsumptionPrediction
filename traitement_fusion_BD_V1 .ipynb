{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Auteur: Y.s ###########\n",
    "############ Fev 2024 ##############\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import sys\n",
    "import gzip\n",
    "\n",
    "folder_projet_BD = r\"D:\\MesDocuments\\Formation\\DataScientist_PSL\\Projet\\BD\"\n",
    "folder_BD_meteo = os.path.join(folder_projet_BD, 'Meteo\\CSV')\n",
    "folder_BD_meteo_region = os.path.join(folder_projet_BD, 'Meteo/region')\n",
    "folder_BD_conso = os.path.join(folder_projet_BD, 'conso-inf36-region')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert \n",
    "# Test de la conversion du champs date en année, mois, jour et heure\n",
    "def extraction_ajout_champs_date(df):\n",
    "    df_info = pd.to_datetime(df_depa_region['AAAAMMJJHH'].astype(str), format =\"%Y%m%d%H\").apply(lambda t: pd.Series({\n",
    "    'date': t.date(),\n",
    "    'year': t.year,\n",
    "    'month': t.month,\n",
    "    'month_n': t.strftime(\"%B\"),\n",
    "    'day': t.day,\n",
    "    'day_n': t.strftime(\"%A\"),\n",
    "    'h': t.hour + 1,\n",
    "    'mn': t.minute,\n",
    "    's': t.second,\n",
    "    }))\n",
    "    df[df_info.columns] = df_info\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggrégation des données météo par région :\n",
    "# ======================================================================================\n",
    "# Nous gardons que les champs suivantes :\n",
    "# NUM_POSTE   \t numéro Météo-France du poste sur 8 chiffres ==> permet d'identier \n",
    "# NOM_USUEL   \t nom usuel du poste\n",
    "# LAT         \t latitude, négative au sud (en degrés et millionièmes de degré)\n",
    "# LON         \t longitude, négative à l’ouest de GREENWICH (en degrés et millionièmes de degré)\n",
    "# ALTI        \t altitude du pied de l'abri ou du pluviomètre si pas d'abri (en m)\n",
    "# AAAAMMJJHH  \t date de la mesure (année mois jour heure)\n",
    "# FF          \t force du vent moyenné sur 10 mn, mesurée à 10 m (en m/s et 1/10)\n",
    "# T           \t température sous abri instantanée (en °C et 1/10)\n",
    "# U           \t humidité relative (en %)\n",
    "# ======================================================================================\n",
    "# Pour \n",
    "    # DIF         \t  rayonnement diffus horaire en heure UTC (en J/cm2) \n",
    "    # DIR         \t  rayonnement direct  horaire en heure UTC (en J/cm2)\n",
    "# Vu qu'ils contiennent beacoup de données manquantes, nous utilsons une autre base avec le rayonnment global à l'échelle d'une région \n",
    "\n",
    "# Dans une région, il y plusieurs postes : \n",
    "# on remplace les mesurs de tous ces postes par les moments statistiques\n",
    "    # d'ordre 1  : moyenne, \n",
    "    # d'ordre 2 : écrat-type, \n",
    "    # d'ordre 3 : asymétrie (skewness), \n",
    "    # d'ordre 4 : l'applatissement (kurtosis )\n",
    "# Fonction pour calculer les moments \n",
    "from scipy.stats import skew\n",
    "from scipy.stats import kurtosis\n",
    "\n",
    "# Les fonctions à appliquer \n",
    "# 25th Percentile\n",
    "def q25(x):\n",
    "    return x.quantile(0.25)\n",
    "# 50th Percentile\n",
    "def q50(x):\n",
    "    return x.quantile(0.5)\n",
    "\n",
    "# 90th Percentile\n",
    "def q75(x):\n",
    "    return x.quantile(0.75)\n",
    "# Suite à un problème avec skew et kurtosis (trop de valeure nan)\n",
    "#aggregation = {\"T\":[(\"moyenne\",'mean'), ('STD', 'std'), ('skew', lambda x : skew(x)),('kurtosis', lambda x : kurtosis(x)) ],\n",
    "#               \"FF\":[(\"moyenne\",'mean'), ('STD', 'std'), ('skew', lambda x : skew(x)),('kurtosis', lambda x : kurtosis(x)) ],\n",
    "#               \"U\":[(\"moyenne\",'mean'), ('STD', 'std'), ('skew', lambda x : skew(x)),('kurtosis', lambda x : kurtosis(x)) ],\n",
    "#               }\n",
    "# Je remplace par les quartiles\n",
    "aggregation = {\"T\":[(\"moyenne\",'mean'), ('STD', 'std'), ('q25', lambda x : q25(x)),('q50', lambda x : q50(x)), ('q75', lambda x : q75(x)) ],\n",
    "               \"FF\":[(\"moyenne\",'mean'), ('STD', 'std'), ('q25', lambda x : q25(x)),('q50', lambda x : q50(x)), ('q75', lambda x : q75(x)) ],\n",
    "               \"U\":[(\"moyenne\",'mean'), ('STD', 'std'), ('q25', lambda x : q25(x)),('q50', lambda x : q50(x)), ('q75', lambda x : q75(x)) ],\n",
    "               }\n",
    "# Téléchargement de la BD avec région et département\n",
    "filename = 'departements-region.csv'\n",
    "file = os.path.join(folder_BD_meteo, filename)\n",
    "df_depa_region = pd.read_csv(file)\n",
    "# JE remplace les espace par un '-' dans les noms des régions\n",
    "df_depa_region['region_name'] = df_depa_region['region_name'].apply(lambda s : s.replace(\" \", '-'))\n",
    "\n",
    "Liste_region = df_depa_region['region_name'].unique().tolist()\n",
    "\n",
    "# df_depa_region['region_name'].unique() # pour vérifer\n",
    "# JE supprime les régions  ['Guadeloupe', 'Martinique', 'Guyane','La-Réunion', 'Mayotte', 'Corse']\n",
    "\n",
    "for elem in ['Guadeloupe', 'Martinique', 'Guyane','La-Réunion', 'Mayotte', 'Corse']:\n",
    "    Liste_region.remove(elem)\n",
    "\n",
    "# Construction d'un dictionnaire de clés région et de valeurs la liste des nuémros de départements de chaque région \n",
    "dictionnaire__departement_region = {}\n",
    "\n",
    "for reg in Liste_region:\n",
    "    dictionnaire__departement_region[reg] = df_depa_region.loc[df_depa_region['region_name']==reg, 'num_dep'].tolist()\n",
    "\n",
    "# Construction de la nouvelle base avec les caractéristiques décrites précédement\n",
    "# Un fichir csv par région pour 2023-2024\n",
    "template_start= 'H_' # début du nom du fichier\n",
    "template_end_2020='_previous-' + str(2020) + '-' + str(2023) + '.csv'\n",
    "template_end_2024='_latest-' + str(2024) + '-' + str(2025) + '.csv'\n",
    "template_end_reg='-' + str(2023) + '-' + str(2024) + '.csv'\n",
    "columns_to_drop =  ['NUM_POSTE','NOM_USUEL','LAT','LON','ALTI','RR1','QRR1','DRR1',\n",
    "                    'QDRR1','QFF','DD','QDD','FXY','QFXY','DXY','QDXY','HXY','QHXY',\n",
    "                    'FXI','QFXI','DXI','QDXI','HXI','QHXI','FF2','QFF2','DD2','QDD2',\n",
    "                    'FXI2','QFXI2','DXI2','QDXI2','HXI2','QHXI2','FXI3S','QFXI3S','DXI3S',\n",
    "                    'QDXI3S','HFXI3S','QHFXI3S','QT','TD','QTD','TN','QTN','HTN','QHTN',\n",
    "                    'TX','QTX','HTX','QHTX','DG','QDG','T10','QT10','T20','QT20','T50',\n",
    "                    'QT50','T100','QT100','TNSOL','QTNSOL','TN50','QTN50','TCHAUSSEE',\n",
    "                    'QTCHAUSSEE','DHUMEC','QDHUMEC','QU','UN','QUN','HUN','QHUN','UX',\n",
    "                    'QUX','HUX','QHUX','DHUMI40','QDHUMI40','DHUMI80','QDHUMI80','TSV','QTSV',\n",
    "                    'PMER','QPMER','PSTAT','QPSTAT','PMERMIN','QPMERMIN','GEOP','QGEOP','N',\n",
    "                    'QN','NBAS','QNBAS','CL','QCL','CM','QCM','CH','QCH','N1','QN1','C1','QC1',\n",
    "                    'B1','QB1','N2','QN2','C2','QC2','B2','QB2','N3','QN3','C3','QC3','B3',\n",
    "                    'QB3','N4','QN4','C4','QC4','B4','QB4','VV','QVV','DVV200','QDVV200',\n",
    "                    'WW','QWW','W1','QW1','W2','QW2','SOL','QSOL','SOLNG','QSOLNG','TMER',\n",
    "                    'QTMER','VVMER','QVVMER','ETATMER','QETATMER','DIRHOULE','QDIRHOULE',\n",
    "                    'HVAGUE','QHVAGUE','PVAGUE','QPVAGUE','HNEIGEF','QHNEIGEF','NEIGETOT',\n",
    "                    'QNEIGETOT','TSNEIGE','QTSNEIGE','TUBENEIGE','QTUBENEIGE','HNEIGEFI3',\n",
    "                    'QHNEIGEFI3','HNEIGEFI1','QHNEIGEFI1','ESNEIGE','QESNEIGE','CHARGENEIGE',\n",
    "                    'QCHARGENEIGE','GLO','QGLO','GLO2','QGLO2','DIR','QDIR','DIR2','QDIR2',\n",
    "                    'DIF','QDIF','DIF2','QDIF2','UV','QUV','UV2','QUV2','UV_INDICE','QUV_INDICE',\n",
    "                    'INFRAR','QINFRAR','INFRAR2','QINFRAR2','INS','QINS','INS2','QINS2','TLAGON',\n",
    "                    'QTLAGON','TVEGETAUX','QTVEGETAUX','ECOULEMENT','QECOULEMENT']\n",
    "columns_to_keep = ['AAAAMMJJHH','T', 'FF', 'U']\n",
    "for reg, list_dep in dictionnaire__departement_region.items():\n",
    "    print(\"... Nous traitons la région\", reg)\n",
    "    premier_departement = True\n",
    "    for dep in list_dep:\n",
    "        # #Lecture de la dataframe avec des données 2020-2023\n",
    "        filename  = f\"{template_start}{dep}{template_end_2020}\" \n",
    "        file = os.path.join(folder_BD_meteo, filename)\n",
    "        \n",
    "        df =  pd.read_csv(file, sep=';')\n",
    "        # suppression des colonnes\n",
    "        df_2023_dep =df[columns_to_keep]\n",
    "\n",
    "        # On ne garde que les lignes de 2023\n",
    "        df_2023_dep = df_2023_dep.loc[df_2023_dep['AAAAMMJJHH']>=2023010100]\n",
    "\n",
    "    # Lecture des fichiers 2024-2025\n",
    "        filename  = f\"{template_start}{dep}{template_end_2024}\" \n",
    "        file = os.path.join(folder_BD_meteo, filename)\n",
    "        df_2024_dep =  pd.read_csv(file, sep=';')\n",
    "        # chargement \n",
    "        df =  pd.read_csv(file, sep=';')\n",
    "        # suppression des colonnes\n",
    "        df_2024_dep =df[columns_to_keep]\n",
    "\n",
    "        # On ne garde que les lignes de 2024\n",
    "        df_2024_dep = df_2024_dep.loc[df_2024_dep['AAAAMMJJHH']<2025010100]\n",
    "\n",
    "      \n",
    "\n",
    "        # Union des deux df 2023 et 2024  avec la df de la région\n",
    "        if premier_departement == True:\n",
    "            df_2023_2024 = pd.concat([df_2023_dep, df_2024_dep], ignore_index=True)\n",
    "            premier_departement = False \n",
    "        else:\n",
    "            df_2023_2024 = pd.concat([df_2023_2024,df_2023_dep, df_2024_dep], ignore_index=True)\n",
    "    # Maintenant on a un df de la région avec les champs souhaités\n",
    "    # On applique les stats \n",
    "    df_stat_2023_2024 = df_2023_2024.groupby('AAAAMMJJHH').agg(aggregation)\n",
    "    df_stat_2023_2024.columns = df_stat_2023_2024.columns.map('_'.join)\n",
    "    df_stat_2023_2024.reset_index(inplace=True) # reindexation\n",
    "\n",
    "    # Extraction et ajout des champs année, mois, jours, heure\n",
    "    df_info = pd.to_datetime(df_stat_2023_2024['AAAAMMJJHH'].astype(str), \n",
    "                                 format =\"%Y%m%d%H\").apply(lambda t: pd.Series({'date': t.date(),\n",
    "                                                                                'year': t.year,\n",
    "                                                                                'month': t.month,\n",
    "                                                                                'month_n': t.strftime(\"%B\"),\n",
    "                                                                                'day': t.day,\n",
    "                                                                                'day_n': t.strftime(\"%A\"),\n",
    "                                                                                'h': t.hour + 1,\n",
    "                                                                                'mn': t.minute,\n",
    "                                                                                's': t.second,\n",
    "                                                                                }))\n",
    "    df_stat_2023_2024[df_info.columns] = df_info\n",
    "\n",
    "    # On sauvegarde dans un CSV : nom   = H_region-2023-2024.csv\" \n",
    "    filename = f\"{template_start}{reg}{template_end_reg}\" \n",
    "    file = os.path.join(folder_BD_meteo_region, filename)\n",
    "    df_stat_2023_2024.to_csv(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concaténation des deux bases de données conso + météo \n",
    "# Pour la consommation \n",
    "template_start_in_meteo= 'H_' # début du nom du fichier meteo \n",
    "template_start_in_conso = 'conso-inf36-' # début du nom du fichier conso  \n",
    "template_end_in='-' + str(2023) + '-' + str(2024) + '.csv'\n",
    "\n",
    "# Pour le fichier csv d'unification des BD\n",
    "template_start_out= 'conso-inf36-meteo-' # début du nom du fichier conso-meteo\n",
    "template_end_out='-' + str(2023) + '-' + str(2024) + '.csv'\n",
    "\n",
    "# Gestion des repertoires \n",
    "folder_projet_BD = r\"D:\\MesDocuments\\Formation\\DataScientist_PSL\\Projet\\BD\"\n",
    "folder_BD_meteo = os.path.join(folder_projet_BD, 'Meteo\\CSV')\n",
    "folder_BD_meteo_region = os.path.join(folder_projet_BD, 'Meteo/region')\n",
    "folder_BD_conso_region = os.path.join(folder_projet_BD, 'conso-inf36-region')\n",
    "folder_BD_conso_meteo = os.path.join(folder_projet_BD, 'conso-inf36-meteo-region')\n",
    "\n",
    "if not os.path.isdir(folder_BD_conso_meteo):\n",
    "    os.mkdir(folder_BD_conso_meteo)\n",
    "for reg in Liste_region[2:]:\n",
    "#def data_conso_meteo_fusion (reg):\n",
    "    print(\"... Nous traitons la région\", reg)\n",
    "    # Lecture de la base meteo de la région \n",
    "    filename = f\"{template_start_in_meteo}{reg}{template_end_in}\" \n",
    "    file = os.path.join(folder_BD_meteo_region, filename)\n",
    "    # lecture de la BD meteo de la région \n",
    "    df_meteo_region = pd.read_csv(file)\n",
    "    df_meteo_region.drop(columns = ['Unnamed: 0'], axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "    # Lecture de la base meteo de la région \n",
    "    filename = f\"{template_start_in_conso}{reg}{template_end_in}\" \n",
    "    file = os.path.join(folder_BD_conso_region, filename)\n",
    "    # lecture de la BD meteo de la région \n",
    "    df_conso_region = pd.read_csv(file, sep = ';')\n",
    "  \n",
    "    print(\"... Extraction\", reg)\n",
    "# # Extraction et ajout des champs année, mois, jours, heure pour la base conso\n",
    "    df_info = pd.to_datetime(df_conso_region['Horodate'],utc=True).apply(lambda t: pd.Series({\n",
    "        'date': t.date(),\n",
    "        'year': t.year,\n",
    "        'month': t.month,\n",
    "        #'month_n': t.strftime(\"%B\"),\n",
    "        'day': t.day,\n",
    "        #'day_n': t.strftime(\"%A\"),\n",
    "        'h': t.hour + 1,\n",
    "        'mn': t.minute,\n",
    "        #'s': t.second,\n",
    "    }))\n",
    "\n",
    "    df_conso_region[df_info.columns] = df_info\n",
    "    # Ajout d'une colonne 'AAAAMMJJHH' qui va nous servir comme colonne commune pour unifier (merge) les deux bases\n",
    "    df_conso_region['AAAAMMJJHH'] = df_info.h + df_info.day*100+ df_info.month*10000 + df_info.year*1000000\n",
    "\n",
    "    # Suppression des colonnes pour ne pas avoir de doublons\n",
    "    df_conso_region.drop(columns=['year', 'month', 'day', 'h', 'date', 'Horodate'], inplace=True)\n",
    "    #df_meteo_region.drop(columns=['mn','s', 'day_n','month_n'], inplace=True)\n",
    "    df_meteo_region.drop(columns=['mn','s'], inplace=True)\n",
    "\n",
    "    print(\"... Fusion\", reg)\n",
    "    # Fusion des deux bases\n",
    "    df_fusion = df_conso_region.merge(right = df_meteo_region, on =\"AAAAMMJJHH\", how = 'left')\n",
    "    print(\"... Enregistrement dans un csv\", reg)\n",
    "    # Enregistrement dans des csv\n",
    "    filename = f\"{template_start_out}{reg}{template_end_out}\" \n",
    "    file = os.path.join(folder_BD_conso_meteo, filename)\n",
    "    # lecture de la BD meteo de la région \n",
    "    df_fusion.to_csv(file)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
